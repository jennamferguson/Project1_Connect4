{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Assuming the data is in a list of dictionaries as described\n",
    "file_path = 'mallika.pkl'\n",
    "data = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "def preprocess_data(data):\n",
    "    boards = []\n",
    "    labels = []\n",
    "    for item in data:\n",
    "        board = np.array(item['board'])\n",
    "        label = item['recommended_column']\n",
    "        boards.append(board)\n",
    "        labels.append(label)\n",
    "    \n",
    "    boards = np.array(boards)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # One-hot encode labels (7 possible moves)\n",
    "    labels = tf.keras.utils.to_categorical(labels, num_classes=7)\n",
    "    return boards, labels\n",
    "\n",
    "boards, labels = preprocess_data(data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(boards, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.2184 - loss: 1.8685 - val_accuracy: 0.3233 - val_loss: 1.6709\n",
      "Epoch 2/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.3349 - loss: 1.6518 - val_accuracy: 0.3689 - val_loss: 1.5739\n",
      "Epoch 3/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.3671 - loss: 1.5638 - val_accuracy: 0.3983 - val_loss: 1.5025\n",
      "Epoch 4/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4016 - loss: 1.4972 - val_accuracy: 0.4030 - val_loss: 1.4766\n",
      "Epoch 5/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4130 - loss: 1.4537 - val_accuracy: 0.4162 - val_loss: 1.4217\n",
      "Epoch 6/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4364 - loss: 1.4163 - val_accuracy: 0.4273 - val_loss: 1.4241\n",
      "Epoch 7/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4477 - loss: 1.3867 - val_accuracy: 0.4289 - val_loss: 1.3958\n",
      "Epoch 8/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4597 - loss: 1.3626 - val_accuracy: 0.4422 - val_loss: 1.3761\n",
      "Epoch 9/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4679 - loss: 1.3430 - val_accuracy: 0.4436 - val_loss: 1.3797\n",
      "Epoch 10/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4740 - loss: 1.3192 - val_accuracy: 0.4427 - val_loss: 1.3720\n",
      "Epoch 11/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4789 - loss: 1.3079 - val_accuracy: 0.4511 - val_loss: 1.3621\n",
      "Epoch 12/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4905 - loss: 1.2790 - val_accuracy: 0.4548 - val_loss: 1.3558\n",
      "Epoch 13/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4921 - loss: 1.2775 - val_accuracy: 0.4519 - val_loss: 1.3647\n",
      "Epoch 14/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4961 - loss: 1.2599 - val_accuracy: 0.4595 - val_loss: 1.3494\n",
      "Epoch 15/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4952 - loss: 1.2559 - val_accuracy: 0.4487 - val_loss: 1.3507\n",
      "Epoch 16/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5050 - loss: 1.2401 - val_accuracy: 0.4599 - val_loss: 1.3596\n",
      "Epoch 17/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5066 - loss: 1.2448 - val_accuracy: 0.4642 - val_loss: 1.3464\n",
      "Epoch 18/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5109 - loss: 1.2275 - val_accuracy: 0.4603 - val_loss: 1.3652\n",
      "Epoch 19/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5135 - loss: 1.2098 - val_accuracy: 0.4711 - val_loss: 1.3407\n",
      "Epoch 20/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5240 - loss: 1.2032 - val_accuracy: 0.4691 - val_loss: 1.3543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x322e39130>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_cnn():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(6, 7, 2), padding=\"same\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\"),  # Use \"same\" padding\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(7, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "\n",
    "\n",
    "# Train CNN\n",
    "cnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2551 - loss: 1.8353 - val_accuracy: 0.3230 - val_loss: 1.6892\n",
      "Epoch 2/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.3437 - loss: 1.6488 - val_accuracy: 0.3273 - val_loss: 1.6598\n",
      "Epoch 3/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3655 - loss: 1.6032 - val_accuracy: 0.3446 - val_loss: 1.6401\n",
      "Epoch 4/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3770 - loss: 1.5719 - val_accuracy: 0.3543 - val_loss: 1.6230\n",
      "Epoch 5/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3923 - loss: 1.5502 - val_accuracy: 0.3626 - val_loss: 1.6185\n",
      "Epoch 6/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4004 - loss: 1.5278 - val_accuracy: 0.3551 - val_loss: 1.6247\n",
      "Epoch 7/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4029 - loss: 1.5211 - val_accuracy: 0.3564 - val_loss: 1.6233\n",
      "Epoch 8/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4036 - loss: 1.5151 - val_accuracy: 0.3678 - val_loss: 1.6167\n",
      "Epoch 9/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4166 - loss: 1.4938 - val_accuracy: 0.3710 - val_loss: 1.6052\n",
      "Epoch 10/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4210 - loss: 1.4822 - val_accuracy: 0.3743 - val_loss: 1.6170\n",
      "Epoch 11/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4290 - loss: 1.4604 - val_accuracy: 0.3701 - val_loss: 1.6198\n",
      "Epoch 12/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4327 - loss: 1.4603 - val_accuracy: 0.3785 - val_loss: 1.6113\n",
      "Epoch 13/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4334 - loss: 1.4568 - val_accuracy: 0.3686 - val_loss: 1.6202\n",
      "Epoch 14/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4335 - loss: 1.4506 - val_accuracy: 0.3743 - val_loss: 1.6184\n",
      "Epoch 15/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4363 - loss: 1.4454 - val_accuracy: 0.3694 - val_loss: 1.6254\n",
      "Epoch 16/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4393 - loss: 1.4397 - val_accuracy: 0.3689 - val_loss: 1.6256\n",
      "Epoch 17/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4440 - loss: 1.4351 - val_accuracy: 0.3780 - val_loss: 1.6217\n",
      "Epoch 18/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4442 - loss: 1.4241 - val_accuracy: 0.3766 - val_loss: 1.6206\n",
      "Epoch 19/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4456 - loss: 1.4220 - val_accuracy: 0.3735 - val_loss: 1.6212\n",
      "Epoch 20/20\n",
      "\u001b[1m787/787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.4538 - loss: 1.4080 - val_accuracy: 0.3712 - val_loss: 1.6247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x326f3b3e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_transformer():\n",
    "    input_layer = layers.Input(shape=(6, 7, 2))\n",
    "    reshaped = layers.Reshape((42, 2))(input_layer)\n",
    "\n",
    "    # Multi-Head Self-Attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=4, key_dim=32)(reshaped, reshaped)\n",
    "    attn_output = layers.LayerNormalization()(attn_output + reshaped)\n",
    "\n",
    "    # Feed-Forward Network\n",
    "    ffn = layers.Dense(128, activation='relu')(attn_output)\n",
    "    ffn = layers.Dense(2)(ffn)  # Match the shape of the input (42, 2)\n",
    "    ffn_output = layers.LayerNormalization()(ffn + attn_output)\n",
    "\n",
    "    # Classification Head\n",
    "    flattened = layers.Flatten()(ffn_output)\n",
    "    dense = layers.Dense(128, activation='relu')(flattened)\n",
    "    output_layer = layers.Dense(7, activation='softmax')(dense)\n",
    "\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "transformer_model = build_transformer()\n",
    "\n",
    "\n",
    "# Train Transformer\n",
    "transformer_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.4775 - loss: 1.3569\n",
      "Validation Loss: 1.3543078899383545, Validation Accuracy: 0.4690828025341034\n",
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3771 - loss: 1.6158\n",
      "Validation Loss: 1.6246517896652222, Validation Accuracy: 0.3711651563644409\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models\n",
    "def evaluate_models(model, X_val, y_val):\n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "evaluate_models(cnn_model, X_val, y_val)\n",
    "evaluate_models(transformer_model, X_val, y_val)\n",
    "\n",
    "# Further Testing Against MCTS (Placeholder)\n",
    "def test_against_mcts(model):\n",
    "    # Implement MCTS testing logic here\n",
    "    pass\n",
    "test_against_mcts(cnn_model)\n",
    "test_against_mcts(transformer_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
